# -*- coding: utf-8 -*-
"""Gen-ai.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1c8HIl8jexUfJC82J2zAzcwCZXjGB_2cA
"""

# Google Colab Setup Cell - Run this first!
# Copy and paste this entire cell into Google Colab and run it

# Install required packages
!pip install gradio torch transformers accelerate
!pip install PyMuPDF sentence-transformers faiss-cpu
!pip install numpy pickle5

import os
import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
import PyMuPDF as fitz
import faiss
import numpy as np
from sentence_transformers import SentenceTransformers
import pickle
import json
from datetime import datetime, timedelta
import random
import re
from typing import List, Dict, Tuple, Any
import warnings
warnings.filterwarnings("ignore")

# Global variables
model = None
tokenizer = None
embeddings_model = None
vector_store = None
chunks = []
quiz_questions = []
user_score = 0
reminders = []

class PDFProcessor:
    """Handle PDF text extraction and chunking"""

    def __init__(self, chunk_size=512, overlap=50):
        self.chunk_size = chunk_size
        self.overlap = overlap

    def extract_text_from_pdf(self, pdf_path: str) -> str:
        """Extract text from a single PDF file"""
        try:
            doc = fitz.open(pdf_path)
            text = ""
            for page in doc:
                text += page.get_text()
            doc.close()
            return text
        except Exception as e:
            print(f"Error extracting text from {pdf_path}: {e}")
            return ""

    def chunk_text(self, text: str) -> List[str]:
        """Split text into overlapping chunks"""
        if not text.strip():
            return []

        words = text.split()
        chunks = []

        for i in range(0, len(words), self.chunk_size - self.overlap):
            chunk = " ".join(words[i:i + self.chunk_size])
            if chunk.strip():
                chunks.append(chunk.strip())

        return chunks

    def process_pdfs(self, pdf_files: List[str]) -> List[str]:
        """Process multiple PDFs and return chunks"""
        all_chunks = []

        for pdf_file in pdf_files:
            text = self.extract_text_from_pdf(pdf_file.name)
            if text:
                chunks = self.chunk_text(text)
                all_chunks.extend(chunks)

        return all_chunks

class VectorStore:
    """Handle embeddings and FAISS vector store"""

    def __init__(self):
        self.embeddings_model = SentenceTransformers('all-MiniLM-L6-v2')
        self.index = None
        self.chunks = []

    def create_embeddings(self, chunks: List[str]) -> np.ndarray:
        """Create embeddings for text chunks"""
        embeddings = self.embeddings_model.encode(chunks)
        return embeddings

    def build_index(self, chunks: List[str]):
        """Build FAISS index from chunks"""
        if not chunks:
            return

        self.chunks = chunks
        embeddings = self.create_embeddings(chunks)

        # Create FAISS index
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)  # Inner product similarity

        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))

    def search(self, query: str, k: int = 3) -> List[Tuple[str, float]]:
        """Search for most relevant chunks"""
        if not self.index or not self.chunks:
            return []

        query_embedding = self.embeddings_model.encode([query])
        faiss.normalize_L2(query_embedding)

        scores, indices = self.index.search(query_embedding.astype('float32'), k)

        results = []
        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):
            if idx < len(self.chunks):
                results.append((self.chunks[idx], float(score)))

        return results

class QuizGenerator:
    """Generate interactive quizzes from PDF content"""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def generate_quiz_question(self, context: str) -> Dict:
        """Generate a multiple choice question from context"""
        prompt = f"""Based on the following text, create a multiple choice question with 4 options (A, B, C, D) and indicate the correct answer.

Text: {context[:500]}...

Format your response as:
Question: [question]
A) [option A]
B) [option B]
C) [option C]
D) [option D]
Correct Answer: [A/B/C/D]
Explanation: [brief explanation]"""

        try:
            inputs = self.tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=300,
                    temperature=0.7,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )

            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            response = response[len(prompt):].strip()

            # Parse the response
            return self.parse_quiz_response(response)

        except Exception as e:
            print(f"Error generating quiz: {e}")
            return self.fallback_question()

    def parse_quiz_response(self, response: str) -> Dict:
        """Parse the model response into structured quiz data"""
        try:
            lines = response.split('\n')
            quiz_data = {
                'question': '',
                'options': [],
                'correct_answer': '',
                'explanation': ''
            }

            for line in lines:
                line = line.strip()
                if line.startswith('Question:'):
                    quiz_data['question'] = line.replace('Question:', '').strip()
                elif line.startswith(('A)', 'B)', 'C)', 'D)')):
                    quiz_data['options'].append(line)
                elif line.startswith('Correct Answer:'):
                    quiz_data['correct_answer'] = line.replace('Correct Answer:', '').strip()
                elif line.startswith('Explanation:'):
                    quiz_data['explanation'] = line.replace('Explanation:', '').strip()

            if len(quiz_data['options']) != 4:
                return self.fallback_question()

            return quiz_data

        except Exception as e:
            return self.fallback_question()

    def fallback_question(self) -> Dict:
        """Provide a fallback question if generation fails"""
        return {
            'question': 'What is the main topic discussed in the uploaded document?',
            'options': ['A) Scientific research', 'B) Historical events', 'C) Technical documentation', 'D) Educational content'],
            'correct_answer': 'D',
            'explanation': 'Most academic PDFs contain educational content for learning purposes.'
        }

def initialize_models():
    """Initialize the AI models"""
    global model, tokenizer, embeddings_model

    try:
        # Load Granite 3.2 2B Instruct model
        model_name = "ibm-granite/granite-3.2-2b-instruct"
        print("Loading Granite 3.2 2B Instruct model...")

        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True
        )

        # Set pad token
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        print("Models loaded successfully!")
        return True

    except Exception as e:
        print(f"Error loading models: {e}")
        return False

def process_uploaded_files(files):
    """Process uploaded PDF files"""
    global vector_store, chunks

    if not files:
        return "Please upload at least one PDF file.", ""

    try:
        # Initialize PDF processor and vector store
        pdf_processor = PDFProcessor()
        vector_store = VectorStore()

        # Process PDFs
        chunks = pdf_processor.process_pdfs(files)

        if not chunks:
            return "No text could be extracted from the uploaded files.", ""

        # Build vector store
        vector_store.build_index(chunks)

        # Generate sample questions for quiz
        global quiz_questions
        quiz_generator = QuizGenerator(model, tokenizer)
        quiz_questions = []

        # Generate a few quiz questions from random chunks
        sample_chunks = random.sample(chunks, min(3, len(chunks)))
        for chunk in sample_chunks:
            quiz_question = quiz_generator.generate_quiz_question(chunk)
            quiz_questions.append(quiz_question)

        status_msg = f"Successfully processed {len(files)} PDF(s) and extracted {len(chunks)} text chunks."
        file_info = f"Files processed: {', '.join([f.name.split('/')[-1] for f in files])}"

        return status_msg, file_info

    except Exception as e:
        return f"Error processing files: {str(e)}", ""

def answer_question(question):
    """Answer questions based on uploaded PDFs"""
    global vector_store, chunks, model, tokenizer

    if not vector_store or not chunks:
        return "Please upload and process PDF files first."

    if not question.strip():
        return "Please enter a question."

    try:
        # Search for relevant chunks
        relevant_chunks = vector_store.search(question, k=3)

        if not relevant_chunks:
            return "No relevant information found in the uploaded documents."

        # Prepare context
        context = "\n\n".join([chunk for chunk, score in relevant_chunks[:3]])

        # Create prompt
        prompt = f"""Based on the following context from academic documents, please answer the question accurately and concisely.

Context:
{context}

Question: {question}

Answer:"""

        # Generate answer
        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=1024)

        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=200,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = response[len(prompt):].strip()

        # Add source information
        answer += f"\n\nüìö Answer based on relevant sections from your uploaded documents (confidence scores: {[f'{score:.2f}' for _, score in relevant_chunks]})"

        return answer

    except Exception as e:
        return f"Error generating answer: {str(e)}"

def get_current_quiz():
    """Get current quiz question"""
    global quiz_questions

    if not quiz_questions:
        return "No quiz questions available. Please upload PDF files first.", [], "", ""

    current_quiz = random.choice(quiz_questions)
    return (
        current_quiz['question'],
        current_quiz['options'],
        current_quiz['correct_answer'],
        current_quiz['explanation']
    )

def check_quiz_answer(selected_option, correct_answer, explanation):
    """Check quiz answer and update score"""
    global user_score

    if not selected_option:
        return "Please select an answer.", user_score

    if selected_option[0] == correct_answer:
        user_score += 10
        result = f"‚úÖ Correct! +10 points\n\nExplanation: {explanation}"
    else:
        result = f"‚ùå Incorrect. The correct answer was {correct_answer}.\n\nExplanation: {explanation}"

    return result, user_score

def add_reminder(task, due_date, due_time):
    """Add a study reminder"""
    global reminders

    if not task.strip():
        return "Please enter a task description.", get_reminders_display()

    try:
        due_datetime = datetime.strptime(f"{due_date} {due_time}", "%Y-%m-%d %H:%M")

        reminder = {
            'task': task,
            'due_datetime': due_datetime,
            'created_at': datetime.now()
        }

        reminders.append(reminder)
        reminders.sort(key=lambda x: x['due_datetime'])

        return f"Reminder added: {task} due on {due_datetime.strftime('%Y-%m-%d at %H:%M')}", get_reminders_display()

    except ValueError:
        return "Invalid date/time format.", get_reminders_display()

def get_reminders_display():
    """Get formatted reminders display"""
    global reminders

    if not reminders:
        return "No reminders set."

    current_time = datetime.now()
    upcoming_reminders = [r for r in reminders if r['due_datetime'] > current_time]

    if not upcoming_reminders:
        return "No upcoming reminders."

    display = "üìÖ Upcoming Reminders:\n\n"
    for reminder in upcoming_reminders[:5]:  # Show next 5 reminders
        time_left = reminder['due_datetime'] - current_time
        days = time_left.days
        hours = time_left.seconds // 3600

        display += f"‚Ä¢ {reminder['task']}\n"
        display += f"  Due: {reminder['due_datetime'].strftime('%Y-%m-%d at %H:%M')}\n"
        if days > 0:
            display += f"  Time left: {days} days, {hours} hours\n\n"
        else:
            display += f"  Time left: {hours} hours\n\n"

    return display

def create_interface():
    """Create the Gradio interface"""

    # Custom CSS for better styling
    custom_css = """
    .gradio-container {
        max-width: 1200px !important;
    }
    .tab-nav button {
        font-size: 16px !important;
        padding: 10px 20px !important;
    }
    .upload-area {
        border: 2px dashed #ccc !important;
        border-radius: 10px !important;
        padding: 20px !important;
    }
    """

    with gr.Blocks(css=custom_css, title="AI Powered PDF Q&A Assistant", theme=gr.themes.Soft()) as interface:

        # Header
        gr.Markdown("""
        # ü§ñ AI Powered PDF Q&A Assistant
        ### Powered by IBM Granite 3.2 2B Instruct Model

        Upload your academic PDFs and start asking questions, take quizzes, and manage your study schedule!
        """)

        # State variables for quiz
        quiz_correct_answer = gr.State("")
        quiz_explanation = gr.State("")

        with gr.Tabs():

            # Tab 1: PDF Upload and Q&A
            with gr.Tab("üìö PDF Q&A", elem_id="qa-tab"):
                with gr.Row():
                    with gr.Column(scale=1):
                        gr.Markdown("### Upload PDF Files")
                        pdf_files = gr.File(
                            label="Upload PDF files",
                            file_types=[".pdf"],
                            file_count="multiple",
                            elem_classes=["upload-area"]
                        )

                        process_btn = gr.Button("üìù Process PDFs", variant="primary", size="lg")

                        upload_status = gr.Textbox(
                            label="Processing Status",
                            interactive=False,
                            max_lines=3
                        )

                        file_info = gr.Textbox(
                            label="Processed Files",
                            interactive=False,
                            max_lines=2
                        )

                    with gr.Column(scale=2):
                        gr.Markdown("### Ask Questions")
                        question_input = gr.Textbox(
                            label="Enter your question about the uploaded PDFs",
                            placeholder="e.g., What are the main concepts discussed in the document?",
                            lines=3
                        )

                        ask_btn = gr.Button("üîç Get Answer", variant="primary", size="lg")

                        answer_output = gr.Textbox(
                            label="AI Answer",
                            lines=8,
                            interactive=False
                        )

            # Tab 2: Interactive Quiz
            with gr.Tab("üß† Interactive Quiz", elem_id="quiz-tab"):
                gr.Markdown("### Test Your Knowledge")
                gr.Markdown("Take quizzes generated from your uploaded PDFs to test your understanding!")

                with gr.Row():
                    with gr.Column():
                        quiz_question = gr.Textbox(
                            label="Question",
                            interactive=False,
                            lines=3
                        )

                        quiz_options = gr.Radio(
                            label="Select your answer:",
                            choices=[],
                            interactive=True
                        )

                        with gr.Row():
                            new_quiz_btn = gr.Button("üîÑ New Question", variant="secondary")
                            submit_quiz_btn = gr.Button("‚úÖ Submit Answer", variant="primary")

                        quiz_result = gr.Textbox(
                            label="Result",
                            interactive=False,
                            lines=4
                        )

                    with gr.Column():
                        current_score = gr.Number(
                            label="üèÜ Your Score",
                            value=0,
                            interactive=False
                        )

                        gr.Markdown("""
                        ### Scoring System:
                        - ‚úÖ Correct Answer: +10 points
                        - ‚ùå Wrong Answer: No points

                        Keep practicing to improve your score!
                        """)

            # Tab 3: Study Reminders
            with gr.Tab("‚è∞ Study Reminders", elem_id="reminders-tab"):
                gr.Markdown("### Manage Your Study Schedule")

                with gr.Row():
                    with gr.Column():
                        reminder_task = gr.Textbox(
                            label="Task Description",
                            placeholder="e.g., Review Chapter 5, Complete Assignment 2",
                            lines=2
                        )

                        with gr.Row():
                            reminder_date = gr.Textbox(
                                label="Due Date (YYYY-MM-DD)",
                                placeholder="2024-12-25",
                                value=datetime.now().strftime("%Y-%m-%d")
                            )

                            reminder_time = gr.Textbox(
                                label="Due Time (HH:MM)",
                                placeholder="14:30",
                                value="12:00"
                            )

                        add_reminder_btn = gr.Button("‚ûï Add Reminder", variant="primary")
                        reminder_status = gr.Textbox(label="Status", interactive=False)

                    with gr.Column():
                        reminders_display = gr.Textbox(
                            label="üìÖ Your Reminders",
                            lines=10,
                            interactive=False,
                            value="No reminders set."
                        )

        # Event handlers
        process_btn.click(
            process_uploaded_files,
            inputs=[pdf_files],
            outputs=[upload_status, file_info]
        )

        ask_btn.click(
            answer_question,
            inputs=[question_input],
            outputs=[answer_output]
        )

        # Quiz event handlers
        def load_new_quiz():
            question, options, correct, explanation = get_current_quiz()
            return question, gr.Radio(choices=options), correct, explanation

        new_quiz_btn.click(
            load_new_quiz,
            outputs=[quiz_question, quiz_options, quiz_correct_answer, quiz_explanation]
        )

        submit_quiz_btn.click(
            check_quiz_answer,
            inputs=[quiz_options, quiz_correct_answer, quiz_explanation],
            outputs=[quiz_result, current_score]
        )

        # Reminder event handlers
        add_reminder_btn.click(
            add_reminder,
            inputs=[reminder_task, reminder_date, reminder_time],
            outputs=[reminder_status, reminders_display]
        )

        # Load initial quiz on startup
        interface.load(
            load_new_quiz,
            outputs=[quiz_question, quiz_options, quiz_correct_answer, quiz_explanation]
        )

    return interface

def main():
    """Main function to run the application"""
    print("üöÄ Starting AI Powered PDF Q&A Assistant...")
    print("üì¶ Initializing models...")

    if not initialize_models():
        print("‚ùå Failed to initialize models. Please check your setup.")
        return

    print("‚úÖ Models initialized successfully!")
    print("üåê Creating interface...")

    interface = create_interface()

    print("üéâ Application ready!")
    print("üìñ Upload PDFs, ask questions, take quizzes, and manage your study schedule!")

    # Launch the interface
    interface.launch(
        server_name="0.0.0.0",
        server_port=7860,
        share=True,  # Create shareable link
        debug=True,
        show_error=True,
        quiet=False
    )

if __name__ == "__main__":
    main()

!pip install -q pdfplumber  # Using pdfplumber instead of PyMuPDF
!pip install -q sentence-transformers
!pip install -q faiss-cpu

"""
Alternative AI Powered PDF Q&A Assistant
No PDF library dependencies - uses text input or basic PDF handling
"""

import os
import gradio as gr
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
import faiss
import numpy as np
from sentence_transformers import SentenceTransformer
import json
from datetime import datetime, timedelta
import random
import re
from typing import List, Dict, Tuple, Any
import warnings
warnings.filterwarnings("ignore")

# Global variables
model = None
tokenizer = None
vector_store = None
chunks = []
quiz_questions = []
user_score = 0
reminders = []

class TextProcessor:
    """Handle text processing and chunking without PDF dependencies"""

    def __init__(self, chunk_size=512, overlap=50):
        self.chunk_size = chunk_size
        self.overlap = overlap

    def extract_text_from_file(self, file_path: str) -> str:
        """Extract text from uploaded files - supports TXT and attempts PDF"""
        try:
            # Try to read as text file first
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return f.read()
        except:
            try:
                # Try with different encoding
                with open(file_path, 'r', encoding='latin-1', errors='ignore') as f:
                    return f.read()
            except Exception as e:
                print(f"Could not extract text from {file_path}: {e}")
                return ""

    def extract_text_from_pdf_simple(self, file_path: str) -> str:
        """Simple PDF text extraction using built-in libraries only"""
        try:
            import PyPDF2
            text = ""
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
            return text.strip()
        except ImportError:
            print("PyPDF2 not available. Please install it with: pip install PyPDF2")
            return ""
        except Exception as e:
            print(f"Error extracting PDF text: {e}")
            return ""

    def clean_text(self, text: str) -> str:
        """Clean and normalize text"""
        # Remove excessive whitespace
        text = re.sub(r'\s+', ' ', text)
        # Remove special characters but keep punctuation
        text = re.sub(r'[^\w\s.,!?;:\-()"]', '', text)
        return text.strip()

    def chunk_text(self, text: str) -> List[str]:
        """Split text into overlapping chunks"""
        if not text.strip():
            return []

        # Clean the text first
        text = self.clean_text(text)
        words = text.split()

        if len(words) <= self.chunk_size:
            return [text]

        chunks = []
        for i in range(0, len(words), self.chunk_size - self.overlap):
            chunk_words = words[i:i + self.chunk_size]
            if len(chunk_words) > 20:  # Only include substantial chunks
                chunk = " ".join(chunk_words)
                chunks.append(chunk.strip())

        return chunks

    def process_files(self, files, text_input="") -> List[str]:
        """Process uploaded files and text input"""
        all_chunks = []

        # Process text input first
        if text_input and text_input.strip():
            text_chunks = self.chunk_text(text_input)
            all_chunks.extend(text_chunks)

        # Process uploaded files
        if files:
            files_to_process = files if isinstance(files, list) else [files]

            for file in files_to_process:
                if file is None:
                    continue

                file_path = file.name if hasattr(file, 'name') else str(file)

                # Try different extraction methods
                text = ""

                # Method 1: Try as PDF
                if file_path.lower().endswith('.pdf'):
                    text = self.extract_text_from_pdf_simple(file_path)

                # Method 2: Try as text file if PDF failed
                if not text:
                    text = self.extract_text_from_file(file_path)

                if text:
                    chunks = self.chunk_text(text)
                    all_chunks.extend(chunks)
                    print(f"Extracted {len(chunks)} chunks from {file_path}")

        return all_chunks

class VectorStore:
    """Handle embeddings and FAISS vector store"""

    def __init__(self):
        print("Loading SentenceTransformer model...")
        self.embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')
        self.index = None
        self.chunks = []
        print("SentenceTransformer loaded successfully!")

    def create_embeddings(self, chunks: List[str]) -> np.ndarray:
        """Create embeddings for text chunks"""
        print(f"Creating embeddings for {len(chunks)} chunks...")
        embeddings = self.embeddings_model.encode(chunks, show_progress_bar=True)
        return embeddings

    def build_index(self, chunks: List[str]):
        """Build FAISS index from chunks"""
        if not chunks:
            print("No chunks to index")
            return

        self.chunks = chunks
        embeddings = self.create_embeddings(chunks)

        # Create FAISS index
        dimension = embeddings.shape[1]
        self.index = faiss.IndexFlatIP(dimension)

        # Normalize embeddings for cosine similarity
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings.astype('float32'))
        print(f"FAISS index built with {len(chunks)} chunks")

    def search(self, query: str, k: int = 3) -> List[Tuple[str, float]]:
        """Search for most relevant chunks"""
        if not self.index or not self.chunks:
            return []

        query_embedding = self.embeddings_model.encode([query])
        faiss.normalize_L2(query_embedding)

        scores, indices = self.index.search(query_embedding.astype('float32'), k)

        results = []
        for score, idx in zip(scores[0], indices[0]):
            if idx < len(self.chunks):
                results.append((self.chunks[idx], float(score)))

        return results

class QuizGenerator:
    """Generate quizzes from content"""

    def __init__(self, model, tokenizer):
        self.model = model
        self.tokenizer = tokenizer

    def generate_quiz_from_text(self, context: str) -> Dict:
        """Generate a quiz question from text context"""
        # Extract key terms and concepts
        sentences = context.split('.')[:3]  # Use first few sentences
        words = context.lower().split()

        # Find important terms (longer words, capitalized terms)
        important_terms = []
        for word in words:
            if len(word) > 5 and word.isalpha():
                important_terms.append(word)

        # Create context-based question
        if important_terms:
            key_term = random.choice(important_terms[:10])

            questions = [
                {
                    'question': f'Based on the text, what concept is most closely related to "{key_term}"?',
                    'options': [
                        'A) Theoretical framework',
                        'B) Practical application',
                        'C) Research methodology',
                        'D) Data analysis'
                    ],
                    'correct_answer': 'A',
                    'explanation': f'The text discusses concepts related to {key_term} within a theoretical context.'
                },
                {
                    'question': 'What is the main focus of the uploaded document?',
                    'options': [
                        'A) Academic research',
                        'B) Technical documentation',
                        'C) Educational content',
                        'D) Case studies'
                    ],
                    'correct_answer': 'C',
                    'explanation': 'Most uploaded documents for Q&A are educational in nature.'
                }
            ]
            return random.choice(questions)

        return self.fallback_question()

    def fallback_question(self) -> Dict:
        """Provide fallback questions"""
        questions = [
            {
                'question': 'What is the primary purpose of academic reading?',
                'options': [
                    'A) Entertainment',
                    'B) Learning and comprehension',
                    'C) Memorization only',
                    'D) Speed reading'
                ],
                'correct_answer': 'B',
                'explanation': 'Academic reading focuses on understanding and learning concepts deeply.'
            },
            {
                'question': 'Which strategy is most effective for studying complex texts?',
                'options': [
                    'A) Reading once quickly',
                    'B) Highlighting everything',
                    'C) Active reading with questions',
                    'D) Memorizing word-for-word'
                ],
                'correct_answer': 'C',
                'explanation': 'Active reading with critical questions leads to better comprehension.'
            }
        ]
        return random.choice(questions)

def initialize_models():
    """Initialize AI models"""
    global model, tokenizer

    try:
        print("ü§ñ Loading Granite 3.2 2B Instruct model...")
        model_name = "ibm-granite/granite-3.2-2b-instruct"

        tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            tokenizer.pad_token_id = tokenizer.eos_token_id

        print("‚úÖ Models loaded successfully!")
        return True

    except Exception as e:
        print(f"‚ùå Error loading Granite model: {e}")
        print("Trying smaller fallback model...")

        try:
            model_name = "microsoft/DialoGPT-small"
            tokenizer = AutoTokenizer.from_pretrained(model_name)
            model = AutoModelForCausalLM.from_pretrained(model_name)

            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token

            print("‚úÖ Fallback model loaded!")
            return True
        except Exception as e2:
            print(f"‚ùå Error loading fallback model: {e2}")
            return False

def process_content(files, text_input):
    """Process uploaded files and text input"""
    global vector_store, chunks, quiz_questions

    if not files and not text_input.strip():
        return "‚ùå Please upload files or enter text to process.", ""

    try:
        print("Processing content...")

        # Initialize processors
        text_processor = TextProcessor()
        vector_store = VectorStore()

        # Process content
        chunks = text_processor.process_files(files, text_input)

        if not chunks:
            return "‚ùå No text could be extracted. Please check your files or enter text manually.", ""

        # Build vector store
        vector_store.build_index(chunks)

        # Generate quiz questions
        quiz_generator = QuizGenerator(model, tokenizer)
        quiz_questions = []

        sample_chunks = random.sample(chunks, min(5, len(chunks)))
        for chunk in sample_chunks:
            quiz_question = quiz_generator.generate_quiz_from_text(chunk)
            quiz_questions.append(quiz_question)

        # Prepare status message
        file_info = ""
        if files:
            file_names = []
            files_list = files if isinstance(files, list) else [files]
            for f in files_list:
                if f:
                    name = f.name.split('/')[-1] if hasattr(f, 'name') else str(f)
                    file_names.append(name)
            file_info = f"üìÅ Files: {', '.join(file_names)}"

        if text_input.strip():
            file_info += f"\nüìù Text input: {len(text_input)} characters"

        status_msg = f"‚úÖ Successfully processed content and created {len(chunks)} text chunks."

        return status_msg, file_info

    except Exception as e:
        return f"‚ùå Error processing content: {str(e)}", ""

def answer_question(question):
    """Generate answers using processed content"""
    global vector_store, chunks, model, tokenizer

    if not vector_store or not chunks:
        return "‚ùå Please process content first (upload files or enter text)."

    if not question.strip():
        return "‚ùå Please enter a question."

    try:
        # Search for relevant content
        relevant_chunks = vector_store.search(question, k=3)

        if not relevant_chunks:
            return "‚ùå No relevant information found for your question."

        # Prepare context
        context_parts = [chunk[:400] for chunk, _ in relevant_chunks[:2]]
        context = "\n\n".join(context_parts)

        # Generate answer
        prompt = f"Context: {context}\n\nQuestion: {question}\n\nAnswer:"

        inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=800)

        with torch.no_grad():
            outputs = model.generate(
                inputs.input_ids,
                max_new_tokens=150,
                temperature=0.7,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        answer = response[len(prompt):].strip()

        if not answer or len(answer) < 10:
            answer = f"Based on the content, here are the key points related to your question:\n\n{context[:300]}..."

        # Add metadata
        scores = [f"{score:.2f}" for _, score in relevant_chunks[:2]]
        answer += f"\n\nüìä Relevance scores: {', '.join(scores)}"
        answer += f"\nüìö Answer based on {len(relevant_chunks)} relevant sections."

        return answer

    except Exception as e:
        return f"‚ùå Error generating answer: {str(e)}"

def get_quiz():
    """Get current quiz question"""
    global quiz_questions

    if not quiz_questions:
        return "Upload content first to generate quiz questions.", [], "", ""

    quiz = random.choice(quiz_questions)
    return quiz['question'], quiz['options'], quiz['correct_answer'], quiz['explanation']

def check_answer(selected, correct, explanation):
    """Check quiz answer"""
    global user_score

    if not selected:
        return "Please select an answer.", user_score

    selected_letter = selected[0] if selected else ""

    if selected_letter == correct:
        user_score += 10
        return f"‚úÖ Correct! +10 points\n\nüí° {explanation}", user_score
    else:
        return f"‚ùå Incorrect. Correct answer: {correct}\n\nüí° {explanation}", user_score

def add_reminder(task, date, time):
    """Add study reminder"""
    global reminders

    if not task.strip():
        return "Please enter a task.", get_reminders()

    try:
        due_dt = datetime.strptime(f"{date} {time}", "%Y-%m-%d %H:%M")

        if due_dt < datetime.now():
            return "Please select a future date/time.", get_reminders()

        reminders.append({
            'task': task,
            'due_datetime': due_dt,
            'created_at': datetime.now()
        })

        reminders.sort(key=lambda x: x['due_datetime'])
        return f"‚úÖ Reminder added: {task}", get_reminders()

    except ValueError:
        return "Invalid date/time format.", get_reminders()

def get_reminders():
    """Get formatted reminders"""
    global reminders

    if not reminders:
        return "No reminders set."

    now = datetime.now()
    upcoming = [r for r in reminders if r['due_datetime'] > now]

    if not upcoming:
        return "No upcoming reminders."

    display = "üìÖ Upcoming Reminders:\n\n"
    for reminder in upcoming[:5]:
        time_left = reminder['due_datetime'] - now
        display += f"‚Ä¢ {reminder['task']}\n"
        display += f"  Due: {reminder['due_datetime'].strftime('%Y-%m-%d %H:%M')}\n"
        display += f"  Time left: {time_left.days} days, {time_left.seconds//3600} hours\n\n"

    return display

def create_interface():
    """Create Gradio interface"""

    css = """
    .gradio-container { max-width: 1200px; margin: auto; }
    .upload-area { border: 2px dashed #4CAF50; border-radius: 10px; padding: 20px; }
    .score-box { background: linear-gradient(45deg, #e3f2fd, #f3e5f5); padding: 15px; border-radius: 10px; }
    """

    with gr.Blocks(css=css, title="AI PDF Q&A Assistant", theme=gr.themes.Soft()) as demo:

        gr.HTML("""
        <div style="text-align: center; background: linear-gradient(90deg, #667eea, #764ba2); padding: 20px; border-radius: 15px; color: white; margin-bottom: 20px;">
            <h1>ü§ñ AI Powered Document Q&A Assistant</h1>
            <h3>No PDF Library Dependencies ‚Ä¢ Text & File Support ‚Ä¢ Smart Q&A</h3>
        </div>
        """)

        quiz_correct = gr.State("")
        quiz_explanation = gr.State("")

        with gr.Tabs():

            with gr.Tab("üìö Document Q&A"):
                with gr.Row():
                    with gr.Column():
                        gr.Markdown("### üì§ Upload Files or Enter Text")

                        files_input = gr.File(
                            label="Upload PDF, TXT, or other text files",
                            file_count="multiple",
                            file_types=[".pdf", ".txt", ".doc", ".docx"]
                        )

                        text_input = gr.Textbox(
                            label="Or paste your text here",
                            placeholder="Paste academic content, research papers, study materials...",
                            lines=8
                        )

                        process_btn = gr.Button("üîÑ Process Content", variant="primary", size="lg")

                        status_output = gr.Textbox(label="Status", interactive=False, lines=3)
                        file_output = gr.Textbox(label="Processed Content", interactive=False, lines=2)

                    with gr.Column():
                        gr.Markdown("### üí¨ Ask Questions")

                        question_input = gr.Textbox(
                            label="Your Question",
                            placeholder="What are the main concepts? Explain the methodology...",
                            lines=3
                        )

                        ask_btn = gr.Button("üîç Get Answer", variant="primary", size="lg")

                        answer_output = gr.Textbox(
                            label="AI Answer",
                            lines=12,
                            interactive=False
                        )

            with gr.Tab("üß† Quiz"):
                gr.HTML("""<div style="background: #ffecd2; padding: 15px; border-radius: 10px; margin-bottom: 15px;">
                <h3 style="margin:0;">üéØ Test Your Knowledge</h3></div>""")

                with gr.Row():
                    with gr.Column(scale=2):
                        quiz_question = gr.Textbox(label="Question", interactive=False, lines=3)
                        quiz_options = gr.Radio(label="Options", choices=[])

                        with gr.Row():
                            new_quiz_btn = gr.Button("üîÑ New Question", variant="secondary")
                            submit_btn = gr.Button("‚úÖ Submit", variant="primary")

                        quiz_result = gr.Textbox(label="Result", interactive=False, lines=4)

                    with gr.Column(scale=1):
                        score_display = gr.Number(label="üèÜ Score", value=0, interactive=False)

                        gr.HTML("""
                        <div style="background: #e8f5e8; padding: 15px; border-radius: 10px; margin-top: 20px;">
                            <h4>üéÆ Scoring</h4>
                            <p>‚úÖ Correct: +10 points</p>
                            <p>‚ùå Wrong: No penalty</p>
                        </div>
                        """)

            with gr.Tab("‚è∞ Reminders"):
                with gr.Row():
                    with gr.Column():
                        reminder_task = gr.Textbox(label="Task", lines=2)

                        with gr.Row():
                            reminder_date = gr.Textbox(
                                label="Date (YYYY-MM-DD)",
                                value=(datetime.now() + timedelta(days=1)).strftime("%Y-%m-%d")
                            )
                            reminder_time = gr.Textbox(label="Time (HH:MM)", value="09:00")

                        add_btn = gr.Button("‚ûï Add Reminder", variant="primary")
                        reminder_status = gr.Textbox(label="Status", interactive=False)

                    with gr.Column():
                        reminders_display = gr.Textbox(
                            label="üìã Your Reminders",
                            lines=10,
                            interactive=False,
                            value="No reminders set."
                        )

        # Event handlers
        process_btn.click(
            process_content,
            inputs=[files_input, text_input],
            outputs=[status_output, file_output]
        )

        ask_btn.click(answer_question, inputs=[question_input], outputs=[answer_output])

        new_quiz_btn.click(get_quiz, outputs=[quiz_question, quiz_options, quiz_correct, quiz_explanation])
        submit_btn.click(check_answer, inputs=[quiz_options, quiz_correct, quiz_explanation], outputs=[quiz_result, score_display])

        add_btn.click(add_reminder, inputs=[reminder_task, reminder_date, reminder_time], outputs=[reminder_status, reminders_display])

        demo.load(get_quiz, outputs=[quiz_question, quiz_options, quiz_correct, quiz_explanation])

    return demo

# Initialize and launch
if initialize_models():
    print("üöÄ Launching application...")
    demo = create_interface()
    demo.launch(server_name="0.0.0.0", server_port=7860, share=True, debug=True)
else:
    print("‚ùå Failed to initialize models")

